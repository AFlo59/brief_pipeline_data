# Pipeline Data Engineering - NYC Taxi Data

Une pipeline complÃ¨te d'ingestion, traitement et analyse des donnÃ©es de taxis de New York (NYC Taxi Trip Records) avec une infrastructure moderne de data engineering.

## ğŸ—ï¸ Architecture

```
NYC Open Data (2025)
        â”‚
        â”‚ â‘  TÃ©lÃ©chargement
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parquet     â”‚
â”‚   Files      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ â‘¡ Ingestion
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚    Azure     â”‚
â”‚   PostgreSQL â”‚
â”‚   (Brut)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ â‘¢ Clean
        â”‚ 
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DLT Pipeline â”‚ â‘£  DLT Migration
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ â‘¤ DÃ©ploiement
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Azure     â”‚
â”‚  MongoDB     â”‚
â”‚  (NettoyÃ©)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ â‘¥ Automatisation
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub      â”‚
â”‚  Actions     â”‚
â”‚  (Mensuel)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Technologies UtilisÃ©es

- **Python 3.12+** - Langage principal
- **DuckDB** - Base de donnÃ©es analytique locale
- **PostgreSQL** - Base de donnÃ©es relationnelle
- **MongoDB** - Base de donnÃ©es NoSQL
- **Docker & Docker Compose** - Conteneurisation
- **FastAPI** - API REST moderne
- **SQLAlchemy** - ORM Python
- **DLT (Data Load Tool)** - Pipeline de donnÃ©es moderne
- **Azure** - Cloud computing
- **GitHub Actions** - CI/CD et automatisation
- **Semantic Release** - Gestion automatique des versions

## ğŸ“Š Source des DonnÃ©es

- **URL**: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
- **Format**: Fichiers Parquet (Yellow Taxi Trip Records)
- **PÃ©riode**: AnnÃ©e 2025 uniquement
- **Exemple**: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet

### Structure des DonnÃ©es

Colonnes principales :
- `VendorID`: Identifiant du fournisseur
- `tpep_pickup_datetime`: Date/heure de prise en charge
- `tpep_dropoff_datetime`: Date/heure de dÃ©pose
- `passenger_count`: Nombre de passagers
- `trip_distance`: Distance du trajet (miles)
- `fare_amount`: Montant du tarif
- `tip_amount`: Montant du pourboire
- `total_amount`: Montant total
- `payment_type`: Type de paiement

## ğŸ› ï¸ Installation et Utilisation

### PrÃ©requis

- Python 3.12+
- Docker et Docker Compose
- Git

### Installation

1. **Cloner le repository**
```bash
git clone https://github.com/AFlo59/brief_pipeline_data.git
cd brief_pipeline_data
```

2. **Installer les dÃ©pendances**
```bash
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sur Windows
pip install -r requirements.txt
```

3. **TÃ©lÃ©charger les donnÃ©es**
```bash
python src/download_data.py
```

4. **Importer dans DuckDB**
```bash
python src/import_to_duckdb.py
```

5. **DÃ©marrer avec Docker**
```bash
cp .env.example .env
docker-compose up -d
```

## ğŸ“ Structure du Projet

```
brief_pipeline_data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ download_data.py      # TÃ©lÃ©chargement des donnÃ©es NYC
â”‚   â”œâ”€â”€ import_to_duckdb.py   # Import vers DuckDB
â”‚   â”œâ”€â”€ import_to_postgres.py # Import vers PostgreSQL
â”‚   â”œâ”€â”€ database.py          # Configuration SQLAlchemy
â”‚   â””â”€â”€ main.py              # Application FastAPI
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Fichiers Parquet bruts
â”‚   â””â”€â”€ database/            # Base DuckDB locale
â”œâ”€â”€ .github/workflows/       # GitHub Actions
â”œâ”€â”€ docker-compose.yml       # Services Docker
â”œâ”€â”€ Dockerfile              # Image Docker
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â””â”€â”€ pyproject.toml         # Configuration du projet
```

## ğŸ”„ Workflow de DÃ©veloppement

1. **Branches protÃ©gÃ©es**: `main` (production) et `develop` (dÃ©veloppement)
2. **Feature branches**: `feature/nom-de-la-fonctionnalite`
3. **Semantic Release**: Versioning automatique basÃ© sur les commits
4. **Conventional Commits**: `feat:`, `fix:`, `docs:`, `chore:`

## ğŸ“ˆ Roadmap

- [x] **Jour 1**: Configuration Git/GitHub et ingestion des donnÃ©es
- [x] **Jour 2**: Import dans DuckDB et transition vers Docker
- [ ] **Jour 3**: API REST avec FastAPI
- [ ] **Jour 4**: Pipeline DLT moderne
- [ ] **Jour 5**: DÃ©ploiement Azure et automatisation

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'feat: add some AmazingFeature'`)
4. Push vers la branch (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.
